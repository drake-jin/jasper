---
layout: post
title: (스터디) 하둡.araboza - 2
category: hadoop
image: /assets/posts/res/2017-04-30/hadoop-work.png
tags: [hadoop, mapreduce, hdfs]
---
[데이터공작소] 멥리듀스와 하둡 그리고 hdfs에 대해서 알아봅니다.

## Chapter 02 MapReduce

 ![mapreduce work](/assets/posts/res/2017-04-30/mapreduce-work.png)
 
 - 맵리듀스는 데이터 처리를 위한 프로그래밍 모델이다.
 - 하둡은 다양한 언어로 쓰인 맵 리듀스 프로그램을 구동시킬 수 있다.
 - 가장 중요한 것은 맵리듀스 프로그램은 타고난 병행성 Pararrel을 내포하고 있다.
 - 맵리듀스는 커다란 데이터셋에서 그 진가가 드러난다.

## 하둡 정의
 - 하둡은 대용량 데이터를 분산처리할 수 있게 도와주는 아파치 오픈소스 프록젝트
 - 하둡은 자바위에서 돌아간다.
 - 하둡은 HDFS,MapReduce의 마스터와 슬레이브가 같은 곳에 놓인다.
 
### 하둡의 특징
 1. 오픈소스
 2. 데이터가 있는 곳으로 코드를 이동시킴
 3. 스케일 아웃(스케일 아웃)
 4. 오프라인 배치 프로세싱에 최적화 

### 하둡의 아키텍쳐(1.x 기준)
 1. HDFS나 MapReduce 모두 하나의 마스터와 다수의 슬레이브로 구성되어있습니다.
 2. 마스터 슬레이브 형태의 아키텍쳐를 가진다.

### 하둡 구성 요소

 ![hadoop element](/assets/posts/res/2017-04-30/hadoop-work.png)

 - 네임노드 = 마스터, HDFS 1.x 에는 단하나만 존재하는 네임노드는 hdfs에 저장되는 각종 파일과 디렉토리들의 메타정보를 관리하고 실제 데이터는 다수의 데이터 노드에 분산저장하는 역할을 담당한다.
 - 네임노드 메타 정보 = 메타정보는 저장된 파일의 이름, 디렉토리, 복제본 수, 블록 ID, 블록별 저장 데이터 노드 이름 과 같은 정보들을 메타정보 메모리에 유지합니다.
 - 데이터노드 = 슬레이브, 실질적인 데이터가 있는곳.
 - 2차 네임노드 = 네임노드에 문제가 생기면 전체 아키텍쳐에 문제가 생기기 때문에 2차 네임노드가 별도로 존재하여 백업한다. 이 행위를 체크포인팅 이라 한다.
 - 네임노드를 되살리는 3요소 = 2차 네임노드, 체크포인트, 에딧로그
 - 잡 트레커 = 네임노드에서의 잡을 수행시키는 역할
 - 잡 트래커 = 잡 트래커는 태스크트래커가 수행할 태스크를 스케줄링하는 역할로 시스템 전체에 모든 잡이 수행되도록 한다
 - 테스크 트래커 = 데이터노드에서 실질적인 작업을 수행하는 역할, 잡 트래커에 의해 태스크를 병렬적으로 수행합니다.
 - 테스크 트래커 = 태스크 트레커는 태스크를 수행하고 각 잡의 전체경과를 하나의 레코드로 유지하는 경과 보고서를 잡 트레커에 보낸다. 만일 태스크가 실패하면 잡 트래커는 그것을 다른 태스크 트래커에게 다시 스케줄 한다.
 - 맵리듀스 잡Job =  클라이언트가 수행하려는 작업 단위, 입력데이터, 맵리듀스 프로그램, 설정 정보로 구성된다 하둡은 잡을 맵 태스트와 리듀스 태스크로 나누어 실행한다.
 - 입력스플릿또는 그냥 스플릿 = 하둡은 맵 리듀스의 잡의 입력을 스플릿으로 불리는 고정크기조각으로 나눈다. 하둡은 각 스플릿마다 하나의 맵 태스크를 생성하고 그 스플릿에 있는 각 레코드를 사용자 정의 맵 함수로 처리한다. 
 
## HDFS 특징

![hdfs working](/assets/posts/res/2017-04-30/hdfsworking.gif)

 - [hdfs상세동작 공홈][hdfs]

[hdfs]: https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html

 - hdfs 는 1버전 기준으로 전체시스템에 하나의 네임노드가 주어진다. 2버전에는 하나의 네임노드가 아니라 다수의 네임노드를 설정할 수 있다. 
 - 파일을 여러개의 블록으로 지정할 수 있습니다. 기본64MB, 프로덕션에서는 128MB 를 많이 사용합니다.
 - 하부 운영체제의 파일 시스템을 그대로 사용한다. 운영체제의 파일시스템이 ext3든 NTFS든 상관하지 않고 네임노드의 관리를 받습니다.
 - 하드웨어가 고장나도 문제를 해결할 수 있습니다. hdfs에서 복제본수(Replication Factor)라 하여 시스템 상황에 맞게 복제본 수를 지정할 수 있습니다.
 - Replication Factor의 존제는 다른 데이터노드가 망가져도 데이터 노드에 있던 데이터 블록들을 복구 시킬 수 있습니다.
 - Write Once Read Many 기본적으로 이미 씌어진 파일의 내용을 바꿀 수 없으며 그러려면 파일 전체를 새로 써야 한다.
 - 다른 시스템의 기본 구성 블록 hdfs에서 호환 좋은 MapReduce와 HBase가 있다.
 - 웹에서 접근 가능합니다 1.0 부터는 WebHDFS라는 RestAPI가 제공되므로 HDFS를 Http인터페이스를 통하여 쉽게 접근할 수 있습니다.
 - 세이프 모드 : 클러스터 내의 데이터노드들로부터 상태와 보유 블록 리스트를 받아서 자신이 가지고 있는 정보와 맞춰본 다음에 복제본 개수들이 맞는지 확인한다. 이 과정동안 외부 요청에 반응하지 않는데 이를 세이프 모드라 한다.
  - 네임노드와 데이터노드간의 통신을 주기적으로 한다. 이를 Heartbeat라 하며 데이터노드는 기본적으로 매 3초마다 heartbeat를 보내야한다. 
  - 네임노드 WebInterface : jetty웹 서버와 jsp 를 이용하여 간단히 구현되어있으며, 네임노드의 현재상황을 보여주기위한 방편으로 간단한 웹 인터페이스를 제공한다.


### hdfs의 읽기 쓰기 
 - [hdfs의 읽기 쓰기 상세설명][hdfsReadWrite]

[hdfsReadWrite]: http://www.guru99.com/learn-hdfs-a-beginners-guide.html

### hdfs의 읽기
 
 ![hdfs read](/assets/posts/res/2017-04-30/hdfs-read.png)


 1. hdfs클라이언트는 먼저 네임노드와 통신하여 자신이 읽고자 하는 파일의 이름을 넘깁니다.
 2. 네임노드로부터 해당 파일의 데이터 플록 위치 리스트를 얻어옵니다.
 3. 각 블록 ID별로 복제본의 개수만큼 데이터노드의 이름이 넘어오게 된다.
 4. 파일의 크기가 데이터플록 크기보다 작다면, 리턴되는 리스트는 한 쌍의 데이터노드와 블록 ID(복제 수 만큼 반복된다.)으로 충분하지만, 클라이언트는 이 때 부터 네임노드의 관섭없이 데이터 노드들과 직접 통신하여 데이터블록을 차례대로 읽어 온다. 
 
### hdfs의 쓰기

 ![hdfs write](/assets/posts/res/2017-04-30/hdfs-write.png)
 
1. HDFS 클라이언트는 먼저 파일을 로컬 파일 시스템에 생성하고 데이터를 hdfs 에 쓰기 시작합니다.
 2. 파일 생성이 끝나거나 크기가 데이터블록의 크기(기본 64MB)보다 크다면 이 때 네임노드와 통신합니다. 네임노드는 이 때 파일 생성 요청을 메모리 메타정보와 editlog에 기록합니다. 
 3. 네임노드는 이 데이터가 저장될 데이터 블록의 ID와 이 블록이 저장될 데이터노드들의 이름을 결정하여 HDFS 클라이언트에게 보내준다.
 4. HDFS 클라이언트는 이중 처 번째 데이터노드에 데이터블록을 쓰면서 복제가 발생해야하는 나머지 데이터노드들의 리스트를 같이 넘깁니다
 5. 첫 번째 데이터노드는 데이터를 HDFS 클ㄹ라이언트로 복사받으면서 두 번째 데이터노드와 접촉하여 그곳으로 복제를 시작한다.
 6. 마지막 데이터노드에서 블록의 복제가 완료되면 이 시점에 해당 데이터블록이 생성 완료된것으로 간주한다. 이 프로세스를 복제 파이프라이닝이라고 한다 (Replication Pipelining)
 7. 파일의 생성이 완료된것이 아니라면 다시 2로 간다.

 > 앞서 hdfs읽기에서 비슷하게 네임노드는 실제 데이터복제 과정에는 직접 참여하지 않으며 hdfs클라이언트가 복제본 개수 만큼의 복제를 직접 시도하지 않고 데이터노드들을 통하여 차례로 수행하는 파이프라인의 형식방식을 체택하고 있다.
 
### hdfs 접근
 1. hdfs라이브러리 사용
 2. 셸 커민드라인을 통한 접근

#### hdfs Federation 
 - 1.x 버전에서는 네임노드가 하나였는대 2.x 버전부터는 다중네임노드를 지원해주는것을 이야기 합니다.
 
#### hdfs파일 블록 
 - 스플릿 : 전체입력을 통째로 처리하는 시간보다 많은 스플릿을 통해 분할된 조각을 처리하는 시간이 더 짧게 걸린다. 따라서 만일 병렬로 스플릿을 처리한다면 스플릿크기가 작을 수록 부하 분산에 더 좋은 효과를 낼 수 있다. 
 - 부하 분산 효과는 스플릿이 세분화 될 수럭 (fine-grained)될 수록 더 커진다.
 - 분할크기를 머 작게하면 분할 관리와 맵 태스크 생성을 위한 오버헤드가 전체 잡 실행 시간을 압도하기 시작한다. 기본적으로 64MB의 HDFS블록을 사용하는 추세이다.

#### 최적의 스플릿 크기가 hdfs블록 크기와 같아야 하는 이유
 - 그 블록 크기가 단일 노드에 저장된다고 확신할 수 있는 가장 큰 입력 크기이기 때문.
 - 만일 스플릿이 두 블록에 걸쳐지면 어떤 HDFS노드도 두 블록을 모두 저장하지 않을 것 이기 때문에 일부 스플릿을 네트워크를 통해 맵 태스크가 수행되는 또 다른 노드로 전송해야 한다.
 - 맵 태스크가 전체 로컬 데이터만을 사용하여 실행할 때에 비하면 훨씬 비효율적이 된다.


#### 네임노드의 수복과정
 1. 2차 네임노드는 주기적으로 네임노드에게 체크포인트를 요청한다.
 2. 네임노드는 메모리에 존재하는 파일시스템 메타정보를 디스크 파일로 써버린다 (FS Image) 이를 체크포인트라 한다.
 3. 디스크에 써버린 파일을 2차 네임노드가 복사해간다
 4. 체크포인트 사이에 혹시 발생할지 모르는 문제에 대비하여 체크포인트 사이의 모든 hdfs 트랜잭션은 모두 Editlog라는 파일에도 수록한다.
 5. 2차 네임노드가 네임노드에게 체크포인트를 요청할 때마다 editlog의 내용은 모두 리셋된다.
 6. 네임노드에 문제가 발생하면, 하둡클러스터의 관리자는 2차 네임노드에서 마지막 체크포인트된 파일 위에 네임노드에 있는 editlog를 리플레이하여 hdfs 을 마지막 최종 상태로 복구하게 된다.
 
 > 주의 사항 
 > 1. 체크포인트 파일, 에딧로그 파일에 문제가 있다면 복구는 불가능
 > 2. 네임노드 복구작업은 기본적으로 오래걸린다. 그리고 기본적으로 안전하지도 않다.
 > 3. 하둡 2.0 에서는 개선포인트로 다중 네임노드를 운영하는것임.  

#### hadoop 2.0의 개선 포인트 HA(High Availability)
 1. 네임노드 HA 는 하둡 0.23의 개선포인트들 중 하나입니다. 네임노드를 다수의 노드로 가져가는 방식을 줄여서 HA라고 한다
 2. 하나의 프라이머리 네임노드, 하나 이상의 스텐바이 네임노드로 구성되어있다. ( 프라이머리 네임노드가 작동하지 않는것이 발견되면 그 때 스텐바이 네임노드가 프라이머리 네임노드로 역할을 대신한다. )
 3. 2번 상황을 고려하여 Zookeeper를 사용한다.
 4. 프라이머리 네임노드와 스텐바이 네임노드의 Hdfs메타데이터 정보공유는 기본적인 체크포인트 요청 및 데이터노드의 heartbeat패킷보고에 포함되어있어 전체적인 상황을 알 수 있게 해줍니다.


## 하둡의 병렬수행
 - 이론적으로 하나의 서버에서 이용할 수 있는 모든 하드웨어 스레드별로 다른 프로세스를 실행시켜 다른 연도별 파일을 처리할 수 있다.
 - 하지만 위 이론에는 문제가 있다.
 - 1. 동일한 크기별로 잡을 나눈다는 것이 항상 쉽고 명백하지 못하다. A, B, C 파일중 A가 1TB이고 나머지는 1KB일 때 BC는 금방읽었지만 A가 아직 다 안 읽혔기 때문에 B와C를 읽는 스레드는 기다리게 된다.
 - 2. 독립적인 프로세스로부터 결과를 조합하는 데 더 많은 처리가 필요할 수 있다. 데이터 청크단위로 최고 기온을 획득하여 최종적으로 그 결과를 조합한다면 연되별 최고 기온은 마지막에 찾게 될 수 있다.
 - 3. 단일 서버의 처리 능력은 한정되어 있다. 

### 병렬처리를 위해 쿼리를 맵리듀스의 잡으로 표현
 1. 소규모 테스트를 해보고, 다수 서버로 구성된 클러스터에서 최종적으로 수행해 볼 수 있다.
 2. 맵단계에서의 입력은 원본 데이터, 리듀스 단계에서의 입력데이터는 맵 단계의 수행으로 도출된 결과 값들이다. 

### 맵 리듀스의 맵과 리듀스의 역할
 - 맵은 키와 벨류로 만드는 과정
 - 리듀스는 기와 벨류를 임의의 과정으로 추합하는 과정이다.
 - 기본적으로 입력데이터는 hdfs에서 쓰고, 처리한 결과의 출력데이터도 hdfs에서 쓴다.

## MapReduce
 - 데이터가 있는 서버로 코드를 전송한다. 소스코드보다 데이터의 크기가 훨씬 크기 때문에 처리할 데이터가 있는 서버로 소스코드를 전송함으로써 실행속도를 빠르게 한다.
 - 데이터를 키/벨류 데이터 셋으로 변환처리한다. 입력데이터를 하나이상의 조각으로 나누고 나눠진 조각에 맞추어서 서버에서 각기 다른 병렬처리하는 것인대 이를 맵 태스크라고 한다. 맵 테스크에서 출력된 최종 처리 과정을 리듀스 태스크라 하며, 하나 이상에서 실행할 수 있다. 
 - ShareNothing아키텍쳐로써 리듀스 태스크는 병렬성이 아주 높다. 맵 태스크들끼리 혹은 리듀스 태스크들끼리 서로 의존성이 없어 각기 자기가 처리할 레코드만 보고 처리하면 되기 때문이다
 - 오프라인 배치처리에 적합. hdfs는 기본적으로 오프라인 배치처리를 위한 시스템이기 때문에 MapReduce 프로그램은 실행을 준비하는대 상대적으로 오랜시간이 걸립니다.

### 잡 트래커
 - Job 이란 MapReduce 프로그램을 잡이라 부른다. 하나의 잡은 보통 하나 이상의 맵 태스크, 리듀스 태스크로 구성된다. 간단히 Task라 부르기도 한다. 
 - 잡트래커는 마스터의 서비스(프로세스)로써 사용자로부터 하둡 잡 실행 요청(MapReduce) 코드가 들어간 Jar파일, 입력데이터 위치, 출력데이터 위치등을 받아 이 잡의 태스크들을 해당 MapReduce 클러스터 내의 태스크 트래커들로 나누어 실행하고 그것들이 종료될 때 까지 관리하는 역할을 한다.
 - 특정 태스크가 실패하면 다른 태스크 트래커에 그 태스크를 다시 실행하기도 하면서 잡Job의 성공적인 실행을 책입지게 됩니다.
 - 입력 데이터와 출력데이터는 반드시 hdfs상에 존재해야한다. 태스크 트래커들은 앞서 언급된 것 처럼 데이터 노드들과 물리적인 서버에 존재하게 된다.
 - 태스크 트래커(Task Tracker) 들은 앞서 언급된 것처럼 데이터노드들과 같은 물리적인 서버에 존재한다.  
 - Task Tracker는 각 태스크마다 별개의 JVM을 할당하고 자식 프로세스로 실행한다.
 - Job Tracker의 기능 중 하나는 Speculative Execution 라는 기능이 있는대, 현저히 느린 태스크를 주시하다가 같은 내용으로 다른 노드에게 작업을 시키는 것.
 - 잡 트래커는 HDFS의 마스터인 네임노드와 대부분 같은 서버에 위치하는데 네임노드와 마찬가지로 MapReduce프레임워크 전체에 단 하나만 존재하기 때문에 하둡 1.x나 이전 버전에서 역시 SPOF(Singloe Point Of Failure)가 됩니다.
 - Hadoop2.0 부터는 잡트래커나 태스크트래커가 아키텍쳐에 존재하지 않고 잡트레커 = (리소스 매니저,에플리케이션 마스터) 태스크 트레커 = 노드 매니저 입니다
 - 네임노드와 데이터노드들처럼 잡 트래커는 주기적으로 태스크 트래커들에게 현재상태에 대한 보고를 받습니다. 이것 역시 HeartBeat라 합니다. 

### 잡 트래커 인터페이스
 - 셸 커멘드
 - 웹 인터페이스

### 잡 스케쥴러 Job Scheduler
![Job scheduler](/assets/posts/res/2017-04-30/job-scheduler.png)
 1. 잡 스케줄러는 클러스터에 여러 잡을 동시에 실행하게 합니다.
 2. TaskScheduler클래스를 구현할 수 있다면 잡 스케줄러를 만들 수 있다.
 3. FIFO Scheduler, Capacity Scheduler, Fair Scheduler


### MapReduce에 적합한 분야

 1. 병렬도가 굉장히 높은 단순 작업
 2. 로그 작업
 3. 머신러닝
 
### 부적합한 곳
 1. 실시간 데이터 스트림 처리
 2. 반복실행이 필요한 곳
 3. MapReduce 작업시에 네트워크 데이터 전송량이 너무큰 경우


## 과제
 - 구글랩이 2003 년에 발표한 'The Google File System' 보기
 - 구글랩이 2004 년에 발표한 'MapReduce: Simplified Data Processing on Large Cluster' 보기

